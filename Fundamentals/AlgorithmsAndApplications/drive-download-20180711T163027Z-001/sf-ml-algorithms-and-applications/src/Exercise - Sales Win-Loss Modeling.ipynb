{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework : Sales Data Modeling Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objectives are to introduce/expand on :\n",
    "1. Problem setting and data prep\n",
    "2. Simple feature vs. target analysis \n",
    "3. Data Cleaning and Processing\n",
    "4. Classification modeling \n",
    "5. Classification model evaluation using ROC & Precision/Recall\n",
    "6. Post-modeling feature analysis using feature importance\n",
    "7. Regression modeling\n",
    "8. Regression modeling evaluation and analysis\n",
    "9. Advanced feature analysis using partial dependence\n",
    "10. A more advanced (and personal) method for performing feature vs. target evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the imports for the rest of the code here.  We need sklearn, pandas, numpy, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pdpbox import pdp # conda doesn't have this, you need to : $>pip install pdpbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data itself comes from a shared google drive folder.  Link it under the working directory where you are launching jupyter from. **Read in the file WA_Fn-UseC_-Sales-Win-Loss.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is sales information for different opportunities, including contextual information such as region, timeframe, client details, and whether or not the opportunity was a success (Won). **Investigate the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Problem Setting \n",
    "\n",
    "Problem setting in Machine Learning involves asking questions about the problem requiring modeling.  Does the model objective make sense given the data?  We're handing you the data and modeling context here.  \n",
    "\n",
    "We would like to model: \n",
    "1. The likelihood of winning a given deal \n",
    "1. The total expected value of a won deal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Feature/Target Plots\n",
    "\n",
    "All supervised learning models require an objective or \"target\" field.  This is the value that the model will try and predict given a set of feature data.  Best practice for modeling involves investigating how a given feature field and the target field relate.  This process helps understand the potential utility of a given feature for predicting the target, as well as identify potential problems in the feature or target.\n",
    "\n",
    "Plot each feature/target interaction (i.e. every field vs. Opportunity Result and every field vs. Opportunity Amount USD)\n",
    "\n",
    "1. Which plot did you use for the comparison?  (How do you handle categorical vs. numeric and categorical vs. numerical?)\n",
    "2. What features look useful for prediction?\n",
    "3. Are there features that could be modified?  (numeric to categorical, etc.)\n",
    "4. What features are leakage?\n",
    "5. What features are noise?\n",
    "6. Imagine this dataset was given to us by engineering.  What other follow up questions might we ask?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INSTRUCTOR CODE\n",
    "# This code simply loops through all of the values and creates simple scatter plots for the Opportunity Amount USD target.\n",
    "# The same approach can be used for Opportunity Result.\n",
    "# Other plotting styles are appropriate, e.g. box plots.  Some of them are not as simple to do in python.\n",
    "\n",
    "target = 'Opportunity Amount USD'\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "for c in dat.columns:\n",
    "    if c == target :\n",
    "        continue\n",
    "    elif (is_numeric_dtype(dat[c])):\n",
    "        dat.plot.scatter(c,target,alpha = 0.5, figsize=(16,9), title=c, color=\"steelblue\")\n",
    "    else:\n",
    "        if (len(dat[c].value_counts()) > 12):\n",
    "            print(\"Too many categories for\", c)\n",
    "            continue\n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.plot(c,target,\"bo\", data=dat, alpha = 0.5, color=\"steelblue\")\n",
    " \n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Data Processing/Cleaning\n",
    "The data above contains categorical data.  Unfortunately, scikit learn does not support categorical data for most of its model types.  As a first step, we will turn categorical lable fields into a series of per-label boolean fields (this process is called binarization).  The cell below will print out the list of original field names as well as the final binarized versions.\n",
    "\n",
    "We also need to drop fields from training because they are not valid measurements (Opportunity Number, which is an ID), they are *[leakage](https://www.kaggle.com/wiki/Leakage)* fields (Opportunity Amount USD, which is only populated if the Opp is won), or are the objective field itself (Opportunity Result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Categorize any numeric field\n",
    "def categorize(dat, field):\n",
    "    dat[field] = dat[field].astype(\"str\")\n",
    "    \n",
    "# Deal Size Category becomes a plain string field\n",
    "categorize(dat, \"Deal Size Category\")\n",
    "\n",
    "# Convert Opportunity Result to a simple boolean vector\n",
    "output = dat['Opportunity Result'] == \"Won\"\n",
    "\n",
    "# Drop all target/leakage fields for Opportunity Result\n",
    "dat_filtered = dat.drop([\"Opportunity Result\",\"Opportunity Number\",\"Opportunity Amount USD\"],axis=1)\n",
    "dat_filtered = pd.get_dummies(dat_filtered)\n",
    "\n",
    "print('***ORIGINAL***\\n', dat.columns)\n",
    "print('\\n')\n",
    "print('***BINARIZED***\\n', dat_filtered.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most modeling tasks, we need to split the data into a train and test set.  We train the model on one set of data, and then evaluate it on another.  Models are always at risk of *[overfitting](https://en.wikipedia.org/wiki/Overfitting)* the data they are trained on.  Evaluating a model on new data gives a better understanding of its potential performance in a real world scenario.\n",
    "\n",
    "The function below splits the filtered data and objective field into train/test counterparts.  The result below shows the shape of the two datasets generated:  ~68K records for training, and ~10K records for evaluation.  \n",
    "\n",
    "There's no hard and fast rule for what ratio to use.  Generally an evaluation set that contains 10%-20% of the original data is a good place to start. You'll also need a minimum number of records for evaluation, which differs depdending on the dataset characteristics.  We usually want a few hundred records for simple linear modeling tasks, up through millions for more complex tasks involving many features (especially in NLP domains).\n",
    "\n",
    "For a more formal evaluation, consider using [cross-fold validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split, train_output, test_output = train_test_split(dat_filtered, output, test_size=10000)\n",
    "print(train_split.shape)\n",
    "print(test_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Classification Modeling\n",
    "Once the data is prepared, training is a snap.  We create the model, and pass in the train/objective data as arguments. **Create a LogisticRegression and train it on the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Classification Evaluation using ROC & Precision/Recall\n",
    "\n",
    "ROC and PR curves are powerful binary classifier evaluation techniques.  The curves represent different combinations of metrics under different sensitivity specifications.\n",
    "\n",
    "The difference between them is apparent by looking at their respective formula components:\n",
    "\n",
    "ROC, comprised of True Positive Rate (TPR) and False Positive Rate (FPR) : \n",
    "\n",
    "\\begin{align}\n",
    "\\mathit{TPR} = \\frac{\\mathit{TP}}{\\mathit{TP}+\\mathit{FN}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathit{FPR} = \\frac{\\mathit{FP}}{\\mathit{TN} + \\mathit{FP}}\n",
    "\\end{align}\n",
    "\n",
    "Precision Recall, comprised of Precision (AKA TPR) and Recall\n",
    "\n",
    "\\begin{align}\n",
    "\\mathit{Precision} = \\frac{\\mathit{TP}}{\\mathit{TP}+\\mathit{FP}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathit{Recall} = \\frac{\\mathit{TP}}{\\mathit{TP} + \\mathit{FN}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create small helper functions for evaluation.  One function creates an *[ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)* curve, which helps understand the model performance across a range of sensitivity settings.  The area of the curve drawn by ROC ([AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)), is a good overall performance metric for a binary classifier.  The other function creates a [precision recall curve](https://en.wikipedia.org/wiki/Precision_and_recall).  The average precision (weighted by the recall count at each point) is given as the *average precision score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(model, data, actual):\n",
    "    probas = model.predict_proba(data)[:,1]\n",
    "    fpr, tpr, thr = roc_curve(actual, probas)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label=\"AUC = %0.2f\" % roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "def pr(model, data, actual):\n",
    "    probas = model.predict_proba(data)[:,1]\n",
    "    precision, recall, _ = precision_recall_curve(actual, probas)\n",
    "    pr_ap = average_precision_score(actual, probas)\n",
    "    plt.plot(recall, precision, 'b', label=\"AVP = %0.2f\" % pr_ap)\n",
    "    plt.title('Precision Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a) ROC/PR for Logistic Regression\n",
    "\n",
    "**Evaluate the LogisticRegression results using both of these plot methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b) ROC/PR for Random Forest\n",
    "**Create and evaluate a random forest model using the plot methods given above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c) Questions on Model Performance for Binary Classification\n",
    "\n",
    "Now that we have two models and two evaluations using two techniques, here are some questions:\n",
    "\n",
    "1. Which model (rf/lr) performed the best?\n",
    "2. Which model would you choose to implement in a production system?\n",
    "3. What would a \"perfect\" ROC curve look like?\n",
    "4. What would a \"perfect\" PR curve look like?\n",
    "5. Why are perfect curves not ideal in model evaluations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Feature Analysis Techniques using Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a model is trained, we can examine it to determine which features it relied on the most during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression have coefficients based on the features they are trained on.  Inspecting these coefficients reveal which features the model believes are important for prediction, and whether those features have a positive or negative impact on the predicted score.  We can look at a sorted list for the best-to-worst indicators of a successful opportunity. **Print out a sorted list of the coefficients in order of their importance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests do not use coefficients.  Instead, they have a series of split criteria scattered across a multitude of decision trees.  We calculate feature importance by measuring the *[information gain](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)* of each split point for its given feature.  If a split point uses a feature to effectively separate a large amount of data, the importance score for that split is added to the feature.  All importance scores are then tallied.  Scikit provides this information as part of the fitted model object.\n",
    "\n",
    "Note that we *cannot* deduce whether the feature has an overall positive or negative effect on the score based on its importance.  Inside a  non-linear model like a random forest, a given feature value could have positive *and* negative effects at multiple parts of a tree, and in multiple trees in the forest. **Print out a sorted list of the importances for the Random forest model you created above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an opportunity is \"won\", the data provides additional information on the amount of money the opportunity is worth.  We can capture this data and model it separately as a linear regression.  We just need to filter for \"won\" opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won = dat[dat[\"Opportunity Result\"] == \"Won\"].drop([\"Opportunity Result\", \"Opportunity Number\"], axis=1)\n",
    "won_output = won[\"Opportunity Amount USD\"]\n",
    "won = won.drop([\"Opportunity Amount USD\"], axis=1)\n",
    "won = pd.get_dummies(won)\n",
    "won_train, won_test, won_train_output, won_test_output = train_test_split(won, won_output, test_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "linreg = regr.fit(won_train, won_train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Regression modeling evaluation and analysis\n",
    "\n",
    "For regression analysis, the model predicts a value that should be \"close\" to the actual value.  The difference between these values is an individual error measurement.  The individual error measurements can be averaged over a test dataset to form an understanding of the error range for a given model.  It's usually a good idea to compare a mean absolute/squared error to the mean of the value within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linreg.predict(won_test)\n",
    "y_actual = won_test_output\n",
    "print(\"mae : $\" , mean_absolute_error(y_actual, y_pred))\n",
    "# print(\"mse : $\" , mean_squared_error(y_actual, y_pred))\n",
    "print(\"mean: $\", np.mean(y_actual ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dig deeper into the results of the model by better understanding the distribution of actual values.  To do that, we can create a small histogram generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_histogram(dist, x_label, y_label, main, log_scale=False):\n",
    "    n, bins, patches = plt.hist(dist, 100, normed=1, facecolor='green', alpha=0.75)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(main)\n",
    "    plt.grid(True)\n",
    "    if log_scale:\n",
    "        plt.yscale('log', nonposy='clip')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the histogram of opportunity amounts.  This looks like a fairly standard distribution with an exponential fall-off.  We can look at this information in original or log scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_histogram(y_actual, 'USD','Probability','Histogram of Predicted Opportunity Amount USD')\n",
    "gen_histogram(y_actual, 'USD','Probability','(Log) Histogram of Predicted Opportunity Amount USD', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the distribution of *residuals* from the model.  The residuals are the predicted amounts subtracted from the original opportunity amounts.\n",
    "\n",
    "The histogram shows that most of the errors are centered around 0, which is a good sign.  The distribution looks a bit bimodal... or rather trimodal.  Finding lumps of errors and/or misclassifications like this typically can highlight records that the model gets consistently wrong.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = y_actual - y_pred\n",
    "gen_histogram(residual, 'Error','Probability','Histogram of Residual Predicted Opportunity Amount USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression coefficients can be interpreted more or less the same as logistic regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef = pd.DataFrame({\"coef\" : linreg.coef_.tolist()},index=won_train.columns)\n",
    "coef.sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8a) Regression Modeling using Random Forests\n",
    "**Create a random forest model and evaluate it using the same approach as the linear regressor. Train it, generate predictions for your test data, calculate the MAE and mean for the predictions versus the test data.**\n",
    "\n",
    "1. Which model performs the best?\n",
    "2. Compare the residuals from both models, are there any differences?\n",
    "3. Which model would you chose for production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR CODE\n",
    "residual = y_actual - y_pred\n",
    "gen_histogram(residual, 'Error','Probability','Histogram of Residual Predicted Opportunity Amount USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) BONUS: Advanced Feature Analysis With Partial Dependence \n",
    "\n",
    "It's still possible to understand the effects of a given feature range using techniques like *[partial dependence plots](https://towardsdatascience.com/introducing-pdpbox-2aa820afd312)*.  These techniques evaluate the predicted value for the given data, while sweeping a given feature through its entire range.  It's possible to understand if a given feature produces a linear response in the predicted value, or if it suggests a more complex (or non-linear) function.  \n",
    "\n",
    "In this case, we'll look at \"Total Days Identified Through Closing\".  This feature gives the number of days the deal has been active.  It's clear that likelihood for a deal to close increases through its age, but only to a point.  After that point, the likelihood of it closing falls off dramatically.  A more complex distribution like this is not going to be modeled well by a simpler model.  However, the random forest is able to capture this behavior through training.\n",
    "\n",
    "As an aside, scikit learn does not provide a default partial dependence plot function for random forests.  A seperate library is utiltized here.  The ICEplot title refers to the paper and technique for *[Individual Conditional Expectation](https://arxiv.org/abs/1309.6392)*\n",
    "\n",
    "By examining the confidence or output of the model over the range of the feature, we can better understand the impact of the feature on the target field over the range of the feature.  In some cases, features have a *non-linear* effect on the target.  That is, a feature may have a positive effect on the target that changes suddenly over certain ranges.  Partial dependence plots expose this behavior as a simple line chart."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for c in train_split.columns:\n",
    "    pdp_elapsed = pdp.pdp_isolate(rfclf, train_split, c)\n",
    "    pdp.pdp_plot(pdp_elapsed, c, frac_to_plot=5000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Advanced Feature Analysis Mosaic plots : Feature vs. Target\n",
    "\n",
    "Here's a different feature analysis technique that is an extrapolation of [mosaic plots](https://en.wikipedia.org/wiki/Mosaic_plot).\n",
    "\n",
    "This plot shows the relative proportional distribution of datapoints within each feature bin compared to the proportional distribution within the target bin. \n",
    "\n",
    "Bins are derived directly from category fields. They can also be derived from a histogram decomposition of a numeric field.\n",
    "\n",
    "The benefits of this approach include:\n",
    "\n",
    "1. Can be applied to any numeric/categorical feature combination\n",
    "2. Does not suffer from occlusion/outlier skewing\n",
    "3. Gives a good overview of relative importance and proportion of data within difference categories/ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "cat_colormap = plt.cm.get_cmap(\"Pastel2\", 8) #nipy_spectral, Set1,Paired \n",
    "lin_colormap = plt.cm.get_cmap(\"RdYlGn\", 8) #nipy_spectral, Set1,Paired \n",
    "\n",
    "\n",
    "\n",
    "def topn(dat, col, top=7):\n",
    "    top_labels = dat[col].value_counts().sort_values(ascending=False)[0:top].index\n",
    "    topmod = dat[col].map(lambda label: \"NA_TOPN\" if label not in top_labels else label)\n",
    "    return topmod\n",
    "\n",
    "def topbin(dat,col):\n",
    "    if dat[col].dtype == \"object\":\n",
    "        return topn(dat, col).reset_index(drop=True)\n",
    "    else :\n",
    "        counts, bins = np.histogram(dat[col], bins=8) \n",
    "        buckets = bins[np.digitize(dat[col], bins)-1].round(2).astype(\"str\")\n",
    "        col_label = col + \" Bins\"\n",
    "        return pd.Series(data = buckets, name=col_label).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "def umosaic(dat, feature, target, invert_colors=False):\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    datsrt = dat.sort_values([target,feature], ascending=[True,True])\n",
    "    feat = topbin(datsrt, feature)\n",
    "    targ = topbin(datsrt, target)\n",
    "    datmos = pd.DataFrame({feature : feat, target : targ})\n",
    "    \n",
    "    def lab(v):\n",
    "        return v[1]\n",
    "    \n",
    "    def props(v):\n",
    "        value = v[1]\n",
    "        if (dat[target].dtype==\"object\"):\n",
    "            ordered = targ.value_counts().sort_values(ascending=False).index.tolist()\n",
    "            colormap = cat_colormap\n",
    "        else:\n",
    "            ordered = targ.value_counts().index.astype(\"float\").sort_values(ascending=False).tolist()\n",
    "            ordered = [str(o) for o in ordered]\n",
    "            colormap = lin_colormap\n",
    "            \n",
    "        if (invert_colors):\n",
    "            color_dict = {v : len(ordered) - i -1  for i,v in enumerate(ordered)}\n",
    "        else:\n",
    "            color_dict = {v : i  for i,v in enumerate(ordered)}\n",
    "        def adj_trans(c):\n",
    "            return (c[0],c[1],c[2],.5)\n",
    "        \n",
    "        return {\"color\" : adj_trans(colormap(color_dict[value]))}\n",
    "            \n",
    "    mosaic(datmos, [feature, target], properties = props, labelizer=lab, title=\"%s vs. %s\" % (feature, target),ax=ax)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "target = 'Opportunity Result'\n",
    "mdat = dat\n",
    "\n",
    "# target = 'Opportunity Amount USD'\n",
    "# mdat = dat[dat[\"Opportunity Result\"] == \"Won\"]\n",
    "# mdat[target] = np.log10(mdat[target] + 1)\n",
    "\n",
    "for c in mdat.columns:\n",
    "    if c == target:\n",
    "        continue\n",
    "    print(c)\n",
    "    umosaic(mdat, c, target, True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target = 'Opportunity Amount USD'\n",
    "mdat = dat[dat[\"Opportunity Result\"] == \"Won\"]\n",
    "print(\"Non-normalized version\")\n",
    "umosaic(mdat, mdat.columns[1], target, True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalized versions\")\n",
    "mdat[target] = np.log10(mdat[target] + 1)\n",
    "for c in mdat.columns:\n",
    "    if c == target:\n",
    "        continue\n",
    "    print(c)\n",
    "    umosaic(mdat, c, target, True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
