{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create a synthetic dataset\n",
    "X, y = make_blobs(random_state=0)\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# instantiate a model and fit it to the training set\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "# evaluate the model on the test set\n",
    "print(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt-text](images/cv.png)\n",
    "* The train/test split can still introduce issues\n",
    "* Cross validation - same data, broken up more times / sliced up different ways\n",
    "* What does this do for us?\n",
    "* Tuneable w/ hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96078431 0.92156863 0.95833333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "iris = load_iris()\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, iris.data, iris.target)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many folds were there by default?\n",
    "* Let's increase that to see if we get better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.96666667 0.93333333 0.9        1.        ]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score: 0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uh-Oh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris labels:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(\"Iris labels:\\n{}\".format(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the data is structured in a certain way (like this) we can have major variation between the folds.\n",
    "* Even with the folding model, the data structure itself can cause issues.\n",
    "* What is the average value of the 2nd fold of this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt-text](images/cv-2.png)\n",
    "* Splits the splits, handling our Uh-Oh scenario\n",
    "* Commonly used in Salesforce data analysis (web search data).\n",
    "* Shuffle also deals with this (more later on that).\n",
    "* What's the tradeoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[1.         0.93333333 0.43333333 0.96666667 0.43333333]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross-validation scores:\\n{}\".format(\n",
    "    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3)\n",
    "print(\"Cross-validation scores:\\n{}\".format(\n",
    "    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What happened between 3 and 5 splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.9  0.96 0.96]\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "print(\"Cross-validation scores:\\n{}\".format(\n",
    "    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What did we do differently? Did it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave One Out Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\n",
    "print(\"Number of cv iterations: \", len(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another type of cross-fold\n",
    "* Train on everything but one sample, rotate and do it again, leaving out a different sample.\n",
    "* What is the tradeoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle Split Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt-text](images/cv-3.png)\n",
    "* Combine shuffle with split with cross validation\n",
    "* The data blender is on \"High\" now.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "shuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\n",
    "print('Cross-validation scores:\\n', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt-text](images/cv-4.png)\n",
    "* Assume we are doing training on faces for face recognition.\n",
    "* Which would be more difficult to train:\n",
    "    * If we train on one person making faces and test on the same person making faces?\n",
    "    * or\n",
    "    * train on one person making faces and test on a different person making faces?\n",
    "\n",
    "* To avoid the \"easy\" case, we create groups in the data (faces from same person), then train on one group / test on a different group.\n",
    "* Goal is to recognize the condition across sample (the smile), not just the individual sample (person A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupKFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "# create synthetic dataset\n",
    "X, y = make_blobs(n_samples=12, random_state=0)\n",
    "# assume the first three samples belong to the same group,\n",
    "# then the next four, etc.\n",
    "groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\n",
    "scores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\n",
    "print(\"Cross-validation scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance - Linear Classifiers\n",
    "![alt-text](images/svc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Parameters\n",
    "* Do we care more about generalizing, or getting things right?\n",
    "* Support Vector Machines hyperparameters C and gamma\n",
    "* C = _cost_ for making errors\n",
    "  * A large C gives you low bias and high variance\n",
    "  * A small C gives you higher bias and lower variance\n",
    "\n",
    "![alt-text](images/svc-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive grid search implementation\n",
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    iris.data, iris.target, random_state=0)\n",
    "print(\"Size of training set: {}   size of test set: {}\".format(\n",
    "         X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters, train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # evaluate the SVC on the test set\n",
    "        score = svm.score(X_test, y_test)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "print(\"Best score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the problem with the above?\n",
    "* what are we doing?\n",
    "* what should we be doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validate-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt-text](images/train-validate-test.png)\n",
    "### Split the data 3 ways now....\n",
    "* Training (to train the model on)\n",
    "* Validation (to refine hyperparameters on)\n",
    "* Test ( to actually test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# split data into train+validation set and test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train+validation set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_trainval, y_trainval, random_state=1)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # evaluate the SVC on the validation set\n",
    "        score = svm.score(X_valid, y_valid)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "            \n",
    "print(best_score, best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(**kwargs):\n",
    "    print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(**{'foo': 'bar', 'debug': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training set: {} size of validation set: {} size of test set: {}\".format(\n",
    "         X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
    "# rebuild a model on the combined training and validation set,\n",
    "# and evaluate it on the test set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "test_score = svm.score(X_test, y_test)\n",
    "print(\"Best score on validation set: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "print(\"Test set score with best parameters: {:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# reference: manual_grid_search_cv\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters,\n",
    "        # train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        # perform cross-validation\n",
    "        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n",
    "        # compute mean cross-validation accuracy\n",
    "        score = np.mean(scores)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "# rebuild a model on the combined training and validation set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "print(best_parameters, score)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(),\n",
    "                param_grid, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     iris.data, iris.target, random_state=0)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
