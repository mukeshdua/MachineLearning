{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Walkthrough: Analyzing Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the imports for the rest of the code here.  We need sklearn, pandas, numpy, and matplotlib\n",
    "* conda doesn't have __`pdpbox`__, so you'll need to __`pip install pdpbox`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pdpbox import pdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data  comes from the shared Google drive folder.  Link it under the working directory from where you launch Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"ml_course_shared/WA_Fn-UseC_-Sales-Win-Loss.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data is sales information for different opportunities, including contextual information such as region, timeframe, client details, and whether or not the opportunity was a success (Won)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data above contains categorical data.  Unfortunately, __`scikit-learn`__ does not support categorical data for most of its model types\n",
    "* As a first step, we will turn categorical label fields into a series of per-label boolean fields (this process is called _binarization_)\n",
    "* The cell below prints out the list of original field names as well as the final binarized versions\n",
    "* We also need to drop fields from training because they are not valid measurements (Opportunity Number, which is an ID), they are *[leakage](https://www.kaggle.com/wiki/Leakage)* fields (Opportunity Amount USD, which is only populated if the Opp is won), or are the objective field itself (Opportunity Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dat['Opportunity Result'] == \"Won\"\n",
    "dat_filtered = dat.drop([\"Opportunity Result\",\"Opportunity Number\",\"Opportunity Amount USD\"],axis=1)\n",
    "dat_filtered = pd.get_dummies(dat_filtered)\n",
    "print('***ORIGINAL***\\n', dat.columns)\n",
    "print('\\n')\n",
    "print('***BINARIZED***\\n', dat_filtered.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For most modeling tasks, we need to split the data into a training set and a test set\n",
    "* We train the model on one set of data, and then evaluate it on another.  Models are always at risk of *[overfitting](https://en.wikipedia.org/wiki/Overfitting)* the data they are trained on\n",
    "* Evaluating a model on new data gives a better understanding of its potential performance in a real world scenario\n",
    "* The function below splits the filtered data and objective field into train/test counterparts\n",
    "* The result below shows the shape of the two datasets generated:  ~68K records for training, and ~10K records for evaluation\n",
    "* There's no hard and fast rule for what ratio to use–generally an evaluation set that contains 10%-20% of the original data is a good place to start\n",
    "* You'll also need a minimum number of records for evaluation, which differs depdending on the dataset characteristics\n",
    "\n",
    "For a more formal evaluation, consider using [cross-fold validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split, train_output, test_output = train_test_split(dat_filtered, output, test_size=10000)\n",
    "print(train_split.shape)\n",
    "print(test_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once the data is prepared, training is a snap\n",
    "* We create the model, and pass in the train/objective data as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logclf = logreg.fit(train_split, train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic regression have coefficients based on the features they are trained on\n",
    "* Inspecting these coefficients reveal which features the model believes are important for prediction, and whether those features have a positive or negative impact on the predicted score\n",
    "* We can look at a sorted list for the best-to-worst indicators of a successful opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef = pd.DataFrame({\"coef\" : logclf.coef_[0].tolist()}, index=train_split.columns)\n",
    "coef.sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We'll create a small helper function for evaluation\n",
    "* This function creates an *[ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)* curve, which helps understand the model performance across a range of sensitivity settings\n",
    "* The area of the curve drawn by ROC ([AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)), is a good overall performance metric for a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(model, data, actual):\n",
    "    probas = model.predict_proba(data)\n",
    "    fpr, tpr, thr = roc_curve(actual, probas[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, 'b', label=\"AUC = %0.2f\" % roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(logclf, test_split, test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating and evaluating the random forest model proceeds the same\n",
    "* Note that the random forest model has a higher AUC than Logistic Regression\n",
    "* On a pure performance basis, random forest is the winner\n",
    "* However, random forest models are more complex than linear models, both in the algorithmic sense, and in the interpretability sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rfclf = rf.fit(train_split,train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(rfclf, test_split, test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random forests do not use coefficients–instead, they have a series of split criteria scattered across a multitude of decision trees\n",
    "* We calculate feature importance by measuring the *[information gain](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)* of each split point for its given feature\n",
    "* If a split point uses a feature to effectively separate a large amount of data, the importance score for that split is added to the feature\n",
    "* All importance scores are then tallied\n",
    "* __`scikit-learn`__ provides this information as part of the fitted model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that we *cannot* deduce whether the feature has an overall positive or negative effect on the score based on its importance\n",
    "* Inside a non-linear model such as a random forest, a given feature value could have positive *and* negative effects at multiple parts of a tree, and in multiple trees in the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = rfclf.feature_importances_\n",
    "pd.DataFrame({\"feature\" : train_split.columns, \"importance\" : importances}).sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It's still possible to understand the effects of a given feature range using techniques like *[partial dependence plots](https://towardsdatascience.com/introducing-pdpbox-2aa820afd312)*\n",
    "* These techniques evaluate the predicted value for the given data, while sweeping a given feature through its entire range\n",
    "* It's possible to understand if a given feature produces a linear response in the predicted value, or if it suggests a more complex (or non-linear) function  \n",
    "* In this case, we'll look at __`Total Days Identified Through Closing`__, i.e., the number of days the deal has been active\n",
    "  * It's clear that likelihood for a deal to close increases through its age, but only to a point\n",
    "  * After that point, the likelihood of it closing falls off dramatically   * A more complex distribution like this is not going to be modeled well by a simpler model\n",
    "  * However, the random forest is able to capture this behavior through training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As an aside, __`scikit-learn`__ does not provide a default partial dependence plot function for random forests; a separate library is utiltized here\n",
    "* The ICEplot title refers to the paper and technique for *[Individual Conditional Expectation](https://arxiv.org/abs/1309.6392)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_elapsed = pdp.pdp_isolate(rfclf, train_split, 'Total Days Identified Through Closing')\n",
    "pdp.pdp_plot(pdp_elapsed, 'Total Days Identified Through Closing', plot_org_pts=True, plot_lines=True, frac_to_plot=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If an opportunity is __`won`__, the data provides additional information on the amount of money the opportunity is worth\n",
    "* We can capture this data and model it separately as a linear regression * We just need to filter for __`won`__ opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won = dat[dat[\"Opportunity Result\"] == \"Won\"].drop([\"Opportunity Result\"], axis=1)\n",
    "won = pd.get_dummies(won)\n",
    "won_output = won[\"Opportunity Amount USD\"]\n",
    "won_filtered = won.drop([\"Opportunity Number\", \"Opportunity Amount USD\"],axis=1)\n",
    "won_train, won_test, won_train_output, won_test_output = train_test_split(won_filtered, won_output, test_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "linreg = regr.fit(won_train, won_train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear regression coefficients can be interpreted more or less the same as logistic regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame({\"coef\" : linreg.coef_.tolist()},index=won_train.columns)\n",
    "coef.sort_values('coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating the model for a linear regression typically involves understanding the error of the prediction\n",
    "* We need to know how \"off\" the model is on average (_mean absolute error_)\n",
    "* It's also helpful to put this number in context by giving the mean of the opportunity amount as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_pred = linreg.predict(won_test)\n",
    "y_actual = won_test_output\n",
    "print(\"mae : $\" , mean_absolute_error(y_actual, y_pred))\n",
    "print(\"mean: $\", np.mean(y_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can dig deeper into the results of the model by better understanding the distribution of actual values\n",
    "  * To do that, we can create a small histogram generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_histogram(dist, x_label, y_label, main, log_scale=False):\n",
    "    n, bins, patches = plt.hist(dist, 100, normed=1, facecolor='green', alpha=0.75)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(main)\n",
    "    plt.grid(True)\n",
    "    if log_scale:\n",
    "        plt.yscale('log', nonposy='clip')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can inspect the histogram of opportunity amounts\n",
    "  * This looks like a fairly standard distribution with an exponential fall-off\n",
    "  * We can examie this information in original or log scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_histogram(y_actual, 'USD','Probability','Histogram of Predicted Opportunity Amount USD')\n",
    "gen_histogram(y_actual, 'USD','Probability','(Log) Histogram of Predicted Opportunity Amount USD', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can examine the distribution of *residuals* from the model\n",
    "  * The residuals are the predicted amounts subtracted from the original opportunity amounts\n",
    "\n",
    "* The histogram shows that most of the errors are centered around 0, which is a good sign\n",
    "  * The distribution looks a bit bimodal... or rather trimodal\n",
    "  * Finding lumps of errors and/or misclassifications like this typically can highlight records that the model gets consistently wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = y_actual - y_pred\n",
    "gen_histogram(residual, 'Error','Probability','Histogram of Residual Predicted Opportunity Amount USD')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
